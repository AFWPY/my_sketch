{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import functools\n",
    "import copy\n",
    "from model.Component_Attention_Module import ComponentAttentionModule\n",
    "\n",
    "from model.VectorQuantizer import  VectorQuantizer\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(Generator,self).__init__()\n",
    "\n",
    "        # codebook使用VQ-VAE进行编码\n",
    "        num_embeddings = 8192 # 嵌入向量数量，过多容易过拟合，过少容易欠拟合\n",
    "        embedding_dim = 512*8*8 # 512*1*1\n",
    "        commitment_cost = 0.25\n",
    "        self.vq = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
    "\n",
    "        # MHA多头注意力机制，输入input和vq生成的Query\n",
    "        self.MHA = ComponentAttentionModule()\n",
    "\n",
    "        # construct unet structure\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True,vq=self.vq,HMA=self.MHA)  # add the innermost layer\n",
    "        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n",
    "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout,nodown=True)\n",
    "        # gradually reduce the number of filters from ngf * 8 to ngf\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n",
    "            unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout,nodown=True)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n",
    "            unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout,nodown=True)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n",
    "            unet_block = UnetSkipConnectionBlock(ngf, ngf, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout,nodown=True)\n",
    "        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)  # add the outermost layer\n",
    "\n",
    "    def forward(self, input_s):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "        return self.model(input_s)\n",
    "\n",
    "class UnetSkipConnectionBlock(nn.Module):\n",
    "    \"\"\"Defines the Unet submodule with skip connection.\n",
    "        X -------------------identity----------------------\n",
    "        |-- downsampling -- |submodule| -- upsampling --|\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False,HMA = None,vq = None,nodown = None):\n",
    "        \"\"\"Construct a Unet submodule with skip connections.\n",
    "\n",
    "        Parameters:\n",
    "            outer_nc (int) -- the number of filters in the outer conv layer\n",
    "            inner_nc (int) -- the number of filters in the inner conv layer\n",
    "            input_nc (int) -- the number of channels in input images/features\n",
    "            submodule (UnetSkipConnectionBlock) -- previously defined submodules\n",
    "            outermost (bool)    -- if this module is the outermost module\n",
    "            innermost (bool)    -- if this module is the innermost module\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers.\n",
    "        \"\"\"\n",
    "        super(UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        self.innermost = innermost\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        if input_nc is None:\n",
    "            input_nc = outer_nc\n",
    "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=use_bias)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = norm_layer(inner_nc)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = norm_layer(outer_nc)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "\n",
    "            self.down_s = nn.Sequential(*[copy.deepcopy(layer) for layer in down])\n",
    "            self.up = nn.Sequential(*up)\n",
    "            self.submodule  = submodule\n",
    "        elif innermost:\n",
    "             # 原有的下采样层\n",
    "            down = [downrelu, downconv]\n",
    "            self.down_s = nn.Sequential(*[copy.deepcopy(layer) for layer in down])       \n",
    "            # 定义上采样层\n",
    "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            self.up = nn.Sequential(*up)\n",
    "            #生成codebook中最相近的vq\n",
    "            self.vq = vq\n",
    "            # MHA多头注意力机制，输入input和vq生成的Query\n",
    "            self.MHA = HMA\n",
    "\n",
    "        elif nodown:\n",
    "            nodownconv = nn.Conv2d(input_nc, inner_nc, kernel_size=3,\n",
    "                     stride=1, padding=1, bias=use_bias)\n",
    "            noupconv = nn.Conv2d(inner_nc * 2, outer_nc, kernel_size=3,\n",
    "                     stride=1, padding=1, bias=use_bias)\n",
    "            \n",
    "            down = [downrelu, nodownconv, downnorm]\n",
    "            up = [uprelu, noupconv, upnorm]\n",
    "            self.down_s = nn.Sequential(*[copy.deepcopy(layer) for layer in down])\n",
    "            self.up = nn.Sequential(*up)\n",
    "            self.submodule  = submodule\n",
    "\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            self.down_s = nn.Sequential(*[copy.deepcopy(layer) for layer in down])\n",
    "            self.up = nn.Sequential(*up)\n",
    "            self.submodule  = submodule\n",
    "\n",
    "\n",
    "    def forward(self, style):\n",
    "        if self.outermost:\n",
    "            down_s = self.down_s(style)\n",
    "            submoduled_s,vq_loss_s= self.submodule(down_s)\n",
    "            up_s = self.up(submoduled_s)\n",
    "            return up_s,vq_loss_s\n",
    "        else:   # add skip connections\n",
    "            if self.innermost:\n",
    "                # 在最内层先进行下采样\n",
    "                down_s = self.down_s(style)            \n",
    "                # 然后并行地执行MHA和VQ操作               \n",
    "                query_s ,vq_loss_s  = self.vq(down_s)\n",
    "                MHA_s = self.MHA(down_s,query_s)\n",
    "                # 执行上采样\n",
    "                up_s = self.up(MHA_s)\n",
    "                return torch.cat([style, up_s], 1),vq_loss_s\n",
    "            else:\n",
    "                down_s = self.down_s(style)\n",
    "                submoduled_s,vq_loss_s = self.submodule(down_s)\n",
    "                up_s = self.up(submoduled_s)\n",
    "                # 对于非最内层，添加skip连接和子模块            \n",
    "                return torch.cat([style, up_s], 1) ,vq_loss_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.image_paths = root\n",
    "        self.imgs = self.read_file(self.image_paths)\n",
    "        self.transform = transform\n",
    "\n",
    "    def read_file(self, path):\n",
    "        \"\"\"从文件夹中读取数据\"\"\"\n",
    "        files_list = os.listdir(path)\n",
    "        file_path_list = [os.path.join(path, img) for img in files_list]\n",
    "        file_path_list.sort()  # 如果你需要特定的顺序则保留这一行\n",
    "        return file_path_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)  # 返回图片列表的长度\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.imgs[index]  # 使用图片列表中的路径\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([32, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# 定义一个转换操作，比如转换成张量并且归一化\n",
    "transform = transforms.Compose([transforms.Resize((256, 256)),\n",
    "                                          transforms.ToTensor()])\n",
    "\n",
    "train_data_root = \"./data/CUHK/trainB\"\n",
    "\n",
    "# 创建数据集\n",
    "dataset = CustomDataset(root=train_data_root, transform=transform)\n",
    "\n",
    "# 创建数据加载器\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "dataiter = iter(dataloader)\n",
    "images = next(dataiter)\n",
    "\n",
    "# 打印图像和标签的形状\n",
    "print('Images shape:', images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Generator(\n",
       "    (vq): VectorQuantizer(\n",
       "      (_embedding): Embedding(8192, 32768)\n",
       "    )\n",
       "    (MHA): ComponentAttentionModule(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x MultiHeadAttention(\n",
       "          (linears_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (linears_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (linears_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (multihead_concat_fc): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (model): UnetSkipConnectionBlock(\n",
       "      (down_s): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (up): Sequential(\n",
       "        (0): ReLU(inplace=True)\n",
       "        (1): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (2): Tanh()\n",
       "      )\n",
       "      (submodule): UnetSkipConnectionBlock(\n",
       "        (down_s): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (up): Sequential(\n",
       "          (0): ReLU(inplace=True)\n",
       "          (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (submodule): UnetSkipConnectionBlock(\n",
       "          (down_s): Sequential(\n",
       "            (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (up): Sequential(\n",
       "            (0): ReLU(inplace=True)\n",
       "            (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (submodule): UnetSkipConnectionBlock(\n",
       "            (down_s): Sequential(\n",
       "              (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "              (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (up): Sequential(\n",
       "              (0): ReLU(inplace=True)\n",
       "              (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (submodule): UnetSkipConnectionBlock(\n",
       "              (down_s): Sequential(\n",
       "                (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "                (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (up): Sequential(\n",
       "                (0): ReLU(inplace=True)\n",
       "                (1): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (submodule): UnetSkipConnectionBlock(\n",
       "                (down_s): Sequential(\n",
       "                  (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "                  (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "                (up): Sequential(\n",
       "                  (0): ReLU(inplace=True)\n",
       "                  (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "                (submodule): UnetSkipConnectionBlock(\n",
       "                  (down_s): Sequential(\n",
       "                    (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "                    (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (up): Sequential(\n",
       "                    (0): ReLU(inplace=True)\n",
       "                    (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (submodule): UnetSkipConnectionBlock(\n",
       "                    (down_s): Sequential(\n",
       "                      (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "                      (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (up): Sequential(\n",
       "                      (0): ReLU(inplace=True)\n",
       "                      (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (submodule): UnetSkipConnectionBlock(\n",
       "                      (down_s): Sequential(\n",
       "                        (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "                        (1): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      )\n",
       "                      (up): Sequential(\n",
       "                        (0): ReLU(inplace=True)\n",
       "                        (1): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                      )\n",
       "                      (submodule): UnetSkipConnectionBlock(\n",
       "                        (down_s): Sequential(\n",
       "                          (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "                          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                        )\n",
       "                        (up): Sequential(\n",
       "                          (0): ReLU(inplace=True)\n",
       "                          (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                        )\n",
       "                        (submodule): UnetSkipConnectionBlock(\n",
       "                          (down_s): Sequential(\n",
       "                            (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "                            (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                          )\n",
       "                          (up): Sequential(\n",
       "                            (0): ReLU(inplace=True)\n",
       "                            (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                          )\n",
       "                          (submodule): UnetSkipConnectionBlock(\n",
       "                            (down_s): Sequential(\n",
       "                              (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "                              (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                              (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                            )\n",
       "                            (up): Sequential(\n",
       "                              (0): ReLU(inplace=True)\n",
       "                              (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                              (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                            )\n",
       "                            (submodule): UnetSkipConnectionBlock(\n",
       "                              (down_s): Sequential(\n",
       "                                (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "                                (1): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                                (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                              )\n",
       "                              (up): Sequential(\n",
       "                                (0): ReLU(inplace=True)\n",
       "                                (1): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                                (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                              )\n",
       "                              (submodule): UnetSkipConnectionBlock(\n",
       "                                (down_s): Sequential(\n",
       "                                  (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "                                  (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                                  (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                                )\n",
       "                                (up): Sequential(\n",
       "                                  (0): ReLU(inplace=True)\n",
       "                                  (1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                                  (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                                )\n",
       "                                (submodule): UnetSkipConnectionBlock(\n",
       "                                  (down_s): Sequential(\n",
       "                                    (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "                                    (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                                    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                                  )\n",
       "                                  (up): Sequential(\n",
       "                                    (0): ReLU(inplace=True)\n",
       "                                    (1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                                    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                                  )\n",
       "                                  (submodule): UnetSkipConnectionBlock(\n",
       "                                    (down_s): Sequential(\n",
       "                                      (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "                                      (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                                      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                                    )\n",
       "                                    (up): Sequential(\n",
       "                                      (0): ReLU(inplace=True)\n",
       "                                      (1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                                      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                                    )\n",
       "                                    (submodule): UnetSkipConnectionBlock(\n",
       "                                      (down_s): Sequential(\n",
       "                                        (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "                                        (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                                      )\n",
       "                                      (up): Sequential(\n",
       "                                        (0): ReLU(inplace=True)\n",
       "                                        (1): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                                        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                                      )\n",
       "                                      (vq): VectorQuantizer(\n",
       "                                        (_embedding): Embedding(8192, 32768)\n",
       "                                      )\n",
       "                                      (MHA): ComponentAttentionModule(\n",
       "                                        (layers): ModuleList(\n",
       "                                          (0-7): 8 x MultiHeadAttention(\n",
       "                                            (linears_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "                                            (linears_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "                                            (linears_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "                                            (multihead_concat_fc): Linear(in_features=512, out_features=512, bias=False)\n",
       "                                            (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "                                          )\n",
       "                                        )\n",
       "                                      )\n",
       "                                    )\n",
       "                                  )\n",
       "                                )\n",
       "                              )\n",
       "                            )\n",
       "                          )\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import optim\n",
    "model = Generator(input_nc=3, output_nc=3,num_downs=8)\n",
    "\n",
    "\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# 提取 model.vq 的参数\n",
    "vq_params = set(model.vq.parameters())\n",
    "# 提取除 model.vq 之外的所有参数\n",
    "other_params = filter(lambda p: p not in vq_params, model.parameters())\n",
    "\n",
    "optimizer_vq = optim.Adam(vq_params, lr=learning_rate, amsgrad=False)\n",
    "optimizer = optim.Adam(other_params, lr=learning_rate, amsgrad=False)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model,device_ids=[0, 1])\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hp/anaconda3/envs/yang/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VQ_loss : 0.19587081670761108; rec_loss : 1.1529916524887085\n",
      " VQ_loss : 5.637734413146973; rec_loss : 0.015834370627999306\n",
      " VQ_loss : 5.242614269256592; rec_loss : 0.009924331679940224\n",
      " VQ_loss : 13.98647689819336; rec_loss : 0.0019414968555793166\n",
      " VQ_loss : 24.64476776123047; rec_loss : 0.0014034243067726493\n",
      " VQ_loss : 22.556791305541992; rec_loss : 0.0009998355526477098\n",
      " VQ_loss : 22.458839416503906; rec_loss : 0.0007405895739793777\n",
      " VQ_loss : 22.465723037719727; rec_loss : 0.0006783397402614355\n",
      " VQ_loss : 22.56629180908203; rec_loss : 0.00060084875440225\n",
      " VQ_loss : 21.150936126708984; rec_loss : 0.0007073736051097512\n",
      " VQ_loss : 22.227703094482422; rec_loss : 0.0004895029123872519\n",
      " VQ_loss : 20.388389587402344; rec_loss : 0.0004844893701374531\n",
      " VQ_loss : 20.69454574584961; rec_loss : 0.00042007374577224255\n",
      " VQ_loss : 20.317289352416992; rec_loss : 0.00038547543226741254\n",
      " VQ_loss : 20.094646453857422; rec_loss : 0.00038067481364123523\n",
      " VQ_loss : 20.127744674682617; rec_loss : 0.0003360950795467943\n",
      " VQ_loss : 19.63553810119629; rec_loss : 0.0003235952463001013\n",
      " VQ_loss : 19.89023780822754; rec_loss : 0.00027020368725061417\n",
      " VQ_loss : 19.552316665649414; rec_loss : 0.00025938820908777416\n",
      " VQ_loss : 19.419139862060547; rec_loss : 0.00028794194804504514\n",
      " VQ_loss : 19.063556671142578; rec_loss : 0.00022053142311051488\n",
      " VQ_loss : 19.18523406982422; rec_loss : 0.00021404499420896173\n",
      " VQ_loss : 18.876850128173828; rec_loss : 0.00020672858227044344\n",
      " VQ_loss : 18.739870071411133; rec_loss : 0.00022185672423802316\n",
      " VQ_loss : 18.99706268310547; rec_loss : 0.0001940749934874475\n",
      " VQ_loss : 18.349140167236328; rec_loss : 0.00019212774350307882\n",
      " VQ_loss : 18.190349578857422; rec_loss : 0.00016838607552926987\n",
      " VQ_loss : 18.15178680419922; rec_loss : 0.00016365048941224813\n",
      " VQ_loss : 18.113479614257812; rec_loss : 0.00015809251635801047\n",
      " VQ_loss : 18.200042724609375; rec_loss : 0.0001467303663957864\n",
      " VQ_loss : 17.803462982177734; rec_loss : 0.00015536554565187544\n",
      " VQ_loss : 17.35614013671875; rec_loss : 0.00013944707461632788\n",
      " VQ_loss : 17.28498077392578; rec_loss : 0.00013853095879312605\n",
      " VQ_loss : 17.550128936767578; rec_loss : 0.00013443530770018697\n",
      " VQ_loss : 17.24062728881836; rec_loss : 0.00013744363968726248\n",
      " VQ_loss : 16.743032455444336; rec_loss : 0.00013618827506434172\n",
      " VQ_loss : 17.034893035888672; rec_loss : 0.00011307325621601194\n",
      " VQ_loss : 16.939266204833984; rec_loss : 0.00012405523739289492\n",
      " VQ_loss : 16.991390228271484; rec_loss : 0.00012194931332487613\n",
      " VQ_loss : 16.76544189453125; rec_loss : 0.00012160307232988998\n",
      " VQ_loss : 16.549705505371094; rec_loss : 0.00010838006710400805\n",
      " VQ_loss : 16.475061416625977; rec_loss : 0.00010918792395386845\n",
      " VQ_loss : 16.17336654663086; rec_loss : 0.00013512661098502576\n",
      " VQ_loss : 16.133495330810547; rec_loss : 9.081012103706598e-05\n",
      " VQ_loss : 16.222122192382812; rec_loss : 9.936380956787616e-05\n",
      " VQ_loss : 15.851089477539062; rec_loss : 8.92278621904552e-05\n",
      " VQ_loss : 16.029258728027344; rec_loss : 8.88186477823183e-05\n",
      " VQ_loss : 16.102731704711914; rec_loss : 8.016166975721717e-05\n",
      " VQ_loss : 16.043621063232422; rec_loss : 9.593390859663486e-05\n",
      " VQ_loss : 15.98171329498291; rec_loss : 8.074724610196427e-05\n",
      " VQ_loss : 16.004074096679688; rec_loss : 7.594555791001767e-05\n",
      " VQ_loss : 15.941363334655762; rec_loss : 9.356161172036082e-05\n",
      " VQ_loss : 15.324407577514648; rec_loss : 8.255823922809213e-05\n",
      " VQ_loss : 15.467979431152344; rec_loss : 8.031222387216985e-05\n",
      " VQ_loss : 15.82532024383545; rec_loss : 7.182256376836449e-05\n",
      " VQ_loss : 15.659887313842773; rec_loss : 7.064856617944315e-05\n",
      " VQ_loss : 15.546438217163086; rec_loss : 6.248436693567783e-05\n",
      " VQ_loss : 15.219998359680176; rec_loss : 7.028097752481699e-05\n",
      " VQ_loss : 15.044319152832031; rec_loss : 6.144042708911002e-05\n",
      " VQ_loss : 15.364006042480469; rec_loss : 6.179478077683598e-05\n",
      " VQ_loss : 14.913219451904297; rec_loss : 5.767377297161147e-05\n",
      " VQ_loss : 14.635714530944824; rec_loss : 6.490959640359506e-05\n",
      " VQ_loss : 15.031379699707031; rec_loss : 6.0167989431647584e-05\n",
      " VQ_loss : 15.32808780670166; rec_loss : 5.976324246148579e-05\n",
      " VQ_loss : 14.706148147583008; rec_loss : 5.8036879636347294e-05\n",
      " VQ_loss : 15.119061470031738; rec_loss : 6.028257485013455e-05\n",
      " VQ_loss : 13.933345794677734; rec_loss : 6.04447559453547e-05\n",
      " VQ_loss : 14.438804626464844; rec_loss : 5.4358864872483537e-05\n",
      " VQ_loss : 14.38394832611084; rec_loss : 5.3343384934123605e-05\n",
      " VQ_loss : 14.660970687866211; rec_loss : 4.7397501475643367e-05\n",
      " VQ_loss : 14.152399063110352; rec_loss : 4.614930003299378e-05\n",
      " VQ_loss : 13.979949951171875; rec_loss : 5.2136001613689587e-05\n",
      " VQ_loss : 13.757617950439453; rec_loss : 4.481853102333844e-05\n",
      " VQ_loss : 14.109317779541016; rec_loss : 4.451236964087002e-05\n",
      " VQ_loss : 13.750476837158203; rec_loss : 4.634077777154744e-05\n",
      " VQ_loss : 13.911334991455078; rec_loss : 4.3584121158346534e-05\n",
      " VQ_loss : 13.561372756958008; rec_loss : 3.844992534141056e-05\n",
      " VQ_loss : 13.688652038574219; rec_loss : 3.782778367167339e-05\n",
      " VQ_loss : 13.46533489227295; rec_loss : 4.441667624632828e-05\n",
      " VQ_loss : 13.871662139892578; rec_loss : 3.786335219047032e-05\n",
      " VQ_loss : 13.810229301452637; rec_loss : 4.8625726776663214e-05\n",
      " VQ_loss : 10.997955322265625; rec_loss : 8.619474829174578e-05\n",
      " VQ_loss : 5.736678600311279; rec_loss : 6.676519114989787e-05\n",
      " VQ_loss : 7.567087650299072; rec_loss : 5.887122097192332e-05\n",
      " VQ_loss : 7.806244850158691; rec_loss : 6.0407408454921097e-05\n",
      " VQ_loss : 7.610200881958008; rec_loss : 5.489090835908428e-05\n",
      " VQ_loss : 9.482553482055664; rec_loss : 4.4548876758199185e-05\n",
      " VQ_loss : 8.166526794433594; rec_loss : 5.417047213995829e-05\n",
      " VQ_loss : 8.116695404052734; rec_loss : 4.4851196435047314e-05\n",
      " VQ_loss : 7.927211761474609; rec_loss : 4.626130248652771e-05\n",
      " VQ_loss : 8.120626449584961; rec_loss : 3.48584508174099e-05\n",
      " VQ_loss : 9.929201126098633; rec_loss : 3.701045352499932e-05\n",
      " VQ_loss : 9.481210708618164; rec_loss : 3.533968992996961e-05\n",
      " VQ_loss : 9.101126670837402; rec_loss : 3.598117473302409e-05\n",
      " VQ_loss : 9.329229354858398; rec_loss : 3.280140663264319e-05\n",
      " VQ_loss : 9.01913833618164; rec_loss : 3.250060399295762e-05\n",
      " VQ_loss : 8.678243637084961; rec_loss : 3.499853119137697e-05\n",
      " VQ_loss : 8.57486343383789; rec_loss : 3.1243143894243985e-05\n",
      " VQ_loss : 8.742242813110352; rec_loss : 3.0378592782653868e-05\n",
      " VQ_loss : 8.112264633178711; rec_loss : 2.9661676308023743e-05\n",
      " VQ_loss : 8.350750923156738; rec_loss : 2.8189815566292964e-05\n",
      " VQ_loss : 8.265769958496094; rec_loss : 2.7276833861833438e-05\n",
      " VQ_loss : 7.7671308517456055; rec_loss : 2.5747733161551878e-05\n",
      " VQ_loss : 7.561496734619141; rec_loss : 2.5880246539600194e-05\n",
      " VQ_loss : 8.451337814331055; rec_loss : 3.569060572772287e-05\n",
      " VQ_loss : 7.714481353759766; rec_loss : 2.4787761503830552e-05\n",
      " VQ_loss : 8.18703556060791; rec_loss : 2.124979073414579e-05\n",
      " VQ_loss : 7.567856311798096; rec_loss : 2.89035579044139e-05\n",
      " VQ_loss : 7.66278076171875; rec_loss : 3.53894974978175e-05\n",
      " VQ_loss : 45.793270111083984; rec_loss : 0.00014128837210591882\n",
      " VQ_loss : 54.008087158203125; rec_loss : 8.746052481001243e-05\n",
      " VQ_loss : 45.8333625793457; rec_loss : 6.0036294598830864e-05\n",
      " VQ_loss : 48.51957702636719; rec_loss : 6.069062146707438e-05\n",
      " VQ_loss : 48.052978515625; rec_loss : 5.433294427348301e-05\n",
      " VQ_loss : 45.624107360839844; rec_loss : 4.193728091195226e-05\n",
      " VQ_loss : 48.271202087402344; rec_loss : 4.272798469173722e-05\n",
      " VQ_loss : 43.53430938720703; rec_loss : 4.067958070663735e-05\n",
      " VQ_loss : 42.093414306640625; rec_loss : 4.254488158039749e-05\n",
      " VQ_loss : 40.74169921875; rec_loss : 3.235489930375479e-05\n",
      " VQ_loss : 42.9390869140625; rec_loss : 2.8313077564234845e-05\n",
      " VQ_loss : 41.44831848144531; rec_loss : 2.9963222914375365e-05\n",
      " VQ_loss : 70.19087982177734; rec_loss : 3.993702557636425e-05\n",
      " VQ_loss : 68.03372955322266; rec_loss : 3.0062778023420833e-05\n",
      " VQ_loss : 67.78853607177734; rec_loss : 2.755869172688108e-05\n",
      " VQ_loss : 65.38265228271484; rec_loss : 2.9555236324085854e-05\n",
      " VQ_loss : 67.23982238769531; rec_loss : 2.3441938537871465e-05\n",
      " VQ_loss : 65.07994079589844; rec_loss : 3.872354864142835e-05\n",
      " VQ_loss : 64.68924713134766; rec_loss : 3.063810072490014e-05\n",
      " VQ_loss : 62.12849426269531; rec_loss : 2.2583630197914317e-05\n",
      " VQ_loss : 62.39894485473633; rec_loss : 2.2088486730353907e-05\n",
      " VQ_loss : 60.536468505859375; rec_loss : 2.233921804872807e-05\n",
      " VQ_loss : 57.8151969909668; rec_loss : 2.2918840841157362e-05\n",
      " VQ_loss : 58.686317443847656; rec_loss : 2.3137410607887432e-05\n",
      " VQ_loss : 57.77735137939453; rec_loss : 2.58264881267678e-05\n",
      " VQ_loss : 54.089752197265625; rec_loss : 1.9291741409688257e-05\n",
      " VQ_loss : 53.86445999145508; rec_loss : 2.2440186512540095e-05\n",
      " VQ_loss : 49.96565246582031; rec_loss : 1.9505630916683003e-05\n",
      " VQ_loss : 50.31110382080078; rec_loss : 1.7595306417206302e-05\n",
      " VQ_loss : 44.660865783691406; rec_loss : 2.3586326278746128e-05\n",
      " VQ_loss : 44.40262985229492; rec_loss : 1.66255485964939e-05\n",
      " VQ_loss : 45.589622497558594; rec_loss : 2.0860752556473017e-05\n",
      " VQ_loss : 44.20478057861328; rec_loss : 2.0552597561618313e-05\n",
      " VQ_loss : 43.62571716308594; rec_loss : 1.734220131766051e-05\n",
      " VQ_loss : 40.58041763305664; rec_loss : 2.0398967535584234e-05\n",
      " VQ_loss : 41.122005462646484; rec_loss : 1.840887125581503e-05\n",
      " VQ_loss : 39.176605224609375; rec_loss : 1.7272761397180147e-05\n",
      " VQ_loss : 39.05876159667969; rec_loss : 1.3689243132830597e-05\n",
      " VQ_loss : 37.383094787597656; rec_loss : 2.2098669433034956e-05\n",
      " VQ_loss : 37.08151626586914; rec_loss : 1.5835377780604176e-05\n",
      " VQ_loss : 35.10276794433594; rec_loss : 2.7876347303390503e-05\n",
      " VQ_loss : 33.4043083190918; rec_loss : 2.0447650967980735e-05\n",
      " VQ_loss : 32.462074279785156; rec_loss : 1.677470027061645e-05\n",
      " VQ_loss : 30.594125747680664; rec_loss : 1.3311610018718056e-05\n",
      " VQ_loss : 31.439607620239258; rec_loss : 2.2434887796407565e-05\n",
      " VQ_loss : 29.718473434448242; rec_loss : 1.2040711226291023e-05\n",
      " VQ_loss : 27.38125228881836; rec_loss : 1.3333539754967205e-05\n",
      " VQ_loss : 24.508787155151367; rec_loss : 1.3880448932468425e-05\n",
      " VQ_loss : 19.942726135253906; rec_loss : 1.313122902502073e-05\n",
      " VQ_loss : 19.024948120117188; rec_loss : 1.612198684597388e-05\n",
      " VQ_loss : 20.080278396606445; rec_loss : 1.235188938153442e-05\n",
      " VQ_loss : 21.56358528137207; rec_loss : 1.3561856576416176e-05\n",
      " VQ_loss : 20.42196273803711; rec_loss : 1.5296020137611777e-05\n",
      " VQ_loss : 19.90662384033203; rec_loss : 1.2240445357747376e-05\n",
      " VQ_loss : 18.313045501708984; rec_loss : 1.790094029274769e-05\n",
      " VQ_loss : 20.383460998535156; rec_loss : 1.398629683535546e-05\n",
      " VQ_loss : 19.60866928100586; rec_loss : 1.1169242497999221e-05\n",
      " VQ_loss : 19.908912658691406; rec_loss : 9.850891729001887e-06\n",
      " VQ_loss : 19.420185089111328; rec_loss : 1.2524876183306333e-05\n",
      " VQ_loss : 21.184024810791016; rec_loss : 1.1634167094598524e-05\n",
      " VQ_loss : 20.023008346557617; rec_loss : 1.0541918527451344e-05\n",
      " VQ_loss : 27.826154708862305; rec_loss : 1.3623370250570588e-05\n",
      " VQ_loss : 28.29657554626465; rec_loss : 1.276421517104609e-05\n",
      " VQ_loss : 30.53820037841797; rec_loss : 1.1854778676934075e-05\n",
      " VQ_loss : 29.71599578857422; rec_loss : 9.99222174868919e-06\n",
      " VQ_loss : 35.18508529663086; rec_loss : 9.442197551834397e-06\n",
      " VQ_loss : 30.675254821777344; rec_loss : 9.662579032010399e-06\n",
      " VQ_loss : 31.045520782470703; rec_loss : 1.0460174053150695e-05\n",
      " VQ_loss : 31.888715744018555; rec_loss : 9.228824637830257e-06\n",
      " VQ_loss : 29.222002029418945; rec_loss : 1.0054582162410952e-05\n",
      " VQ_loss : 34.19432830810547; rec_loss : 1.1046029612771235e-05\n",
      " VQ_loss : 67.23975372314453; rec_loss : 9.551387847750448e-06\n",
      " VQ_loss : 69.76350402832031; rec_loss : 9.3027683760738e-06\n",
      " VQ_loss : 70.63981628417969; rec_loss : 9.740247151057702e-06\n",
      " VQ_loss : 68.29698181152344; rec_loss : 9.362842320115305e-06\n",
      " VQ_loss : 68.8880844116211; rec_loss : 1.1056297807954252e-05\n",
      " VQ_loss : 69.27169799804688; rec_loss : 8.003435141290538e-06\n",
      " VQ_loss : 63.44953918457031; rec_loss : 8.521028576069511e-06\n",
      " VQ_loss : 71.70512390136719; rec_loss : 1.0423824278404936e-05\n",
      " VQ_loss : 69.60296630859375; rec_loss : 8.352118129550945e-06\n",
      " VQ_loss : 75.1847152709961; rec_loss : 7.7884214988444e-06\n",
      " VQ_loss : 75.37049102783203; rec_loss : 9.891566151054576e-06\n",
      " VQ_loss : 75.74806213378906; rec_loss : 1.539480399515014e-05\n",
      " VQ_loss : 76.55268859863281; rec_loss : 1.327522022620542e-05\n",
      " VQ_loss : 74.36578369140625; rec_loss : 7.971710147103295e-06\n",
      " VQ_loss : 81.69068908691406; rec_loss : 7.734568498563021e-06\n",
      " VQ_loss : 76.17462158203125; rec_loss : 6.981793376326095e-06\n",
      " VQ_loss : 76.36576843261719; rec_loss : 7.340936463151593e-06\n",
      " VQ_loss : 71.87274169921875; rec_loss : 7.738168278592639e-06\n",
      " VQ_loss : 75.86827087402344; rec_loss : 8.63595778355375e-06\n",
      " VQ_loss : 66.62528991699219; rec_loss : 8.595302460889798e-06\n",
      " VQ_loss : 71.28446197509766; rec_loss : 7.407127668557223e-06\n",
      " VQ_loss : 69.13496398925781; rec_loss : 6.64290928398259e-06\n",
      " VQ_loss : 63.2338981628418; rec_loss : 8.114562660921365e-06\n",
      " VQ_loss : 54.64784240722656; rec_loss : 6.9016582529002335e-06\n",
      " VQ_loss : 58.54217529296875; rec_loss : 7.215752702904865e-06\n",
      " VQ_loss : 55.70798873901367; rec_loss : 9.574602700013202e-06\n",
      " VQ_loss : 55.71967315673828; rec_loss : 7.814545824658126e-06\n",
      " VQ_loss : 54.58988571166992; rec_loss : 9.378931281389669e-06\n",
      " VQ_loss : 77.78073120117188; rec_loss : 7.506355359510053e-06\n",
      " VQ_loss : 81.85296630859375; rec_loss : 5.88349939789623e-06\n",
      " VQ_loss : 75.65782165527344; rec_loss : 1.222769424202852e-05\n",
      " VQ_loss : 176.42654418945312; rec_loss : 7.887335232226178e-05\n",
      " VQ_loss : 187.2294921875; rec_loss : 6.271533493418247e-05\n",
      " VQ_loss : 172.901611328125; rec_loss : 4.0984810766531155e-05\n",
      " VQ_loss : 163.0833282470703; rec_loss : 3.476694837445393e-05\n",
      " VQ_loss : 157.2763671875; rec_loss : 3.2519201340619475e-05\n",
      " VQ_loss : 158.34547424316406; rec_loss : 2.768236845440697e-05\n",
      " VQ_loss : 149.18283081054688; rec_loss : 3.351452687638812e-05\n",
      " VQ_loss : 151.36436462402344; rec_loss : 2.2255462681641802e-05\n",
      " VQ_loss : 146.75567626953125; rec_loss : 2.1621843188768253e-05\n",
      " VQ_loss : 136.95196533203125; rec_loss : 2.2899621399119496e-05\n",
      " VQ_loss : 136.6685791015625; rec_loss : 2.001511529670097e-05\n",
      " VQ_loss : 138.3760986328125; rec_loss : 1.8362839909968898e-05\n",
      " VQ_loss : 133.29339599609375; rec_loss : 2.0635385226341896e-05\n",
      " VQ_loss : 136.63363647460938; rec_loss : 1.9838475054712035e-05\n",
      " VQ_loss : 130.83935546875; rec_loss : 1.7893671611091122e-05\n",
      " VQ_loss : 131.9575958251953; rec_loss : 1.6327374396496452e-05\n",
      " VQ_loss : 139.20706176757812; rec_loss : 2.121255784004461e-05\n",
      " VQ_loss : 135.88717651367188; rec_loss : 1.4795952665735967e-05\n",
      " VQ_loss : 137.54147338867188; rec_loss : 1.373931718262611e-05\n",
      " VQ_loss : 139.41519165039062; rec_loss : 1.2996947589272168e-05\n",
      " VQ_loss : 138.89968872070312; rec_loss : 1.3572947864304297e-05\n",
      " VQ_loss : 142.16897583007812; rec_loss : 1.6098703781608492e-05\n",
      " VQ_loss : 125.66557312011719; rec_loss : 1.5014038581284694e-05\n",
      " VQ_loss : 124.40910339355469; rec_loss : 1.3242386557976715e-05\n",
      " VQ_loss : 122.21722412109375; rec_loss : 1.2230773791088723e-05\n",
      " VQ_loss : 122.7835693359375; rec_loss : 1.2140435501351021e-05\n",
      " VQ_loss : 116.40035247802734; rec_loss : 1.3363438483793288e-05\n",
      " VQ_loss : 116.49174499511719; rec_loss : 1.2453546332835685e-05\n",
      " VQ_loss : 124.67584228515625; rec_loss : 1.0400332939752843e-05\n",
      " VQ_loss : 128.0532684326172; rec_loss : 1.4039978850632906e-05\n",
      " VQ_loss : 130.15122985839844; rec_loss : 1.0631174518493935e-05\n",
      " VQ_loss : 131.71536254882812; rec_loss : 1.1422688658058178e-05\n",
      " VQ_loss : 129.68499755859375; rec_loss : 1.1628200809354894e-05\n",
      " VQ_loss : 131.5078582763672; rec_loss : 1.2625532690435648e-05\n",
      " VQ_loss : 136.81838989257812; rec_loss : 1.3888367902836762e-05\n",
      " VQ_loss : 134.4488525390625; rec_loss : 1.3048825167061295e-05\n",
      " VQ_loss : 134.27670288085938; rec_loss : 1.1273383279331028e-05\n",
      " VQ_loss : 129.5563507080078; rec_loss : 8.029166565393098e-06\n",
      " VQ_loss : 131.51962280273438; rec_loss : 1.1661046301014721e-05\n",
      " VQ_loss : 123.41178894042969; rec_loss : 8.74837496667169e-06\n",
      " VQ_loss : 124.3372802734375; rec_loss : 1.029254690365633e-05\n",
      " VQ_loss : 126.09577178955078; rec_loss : 8.69813266035635e-06\n",
      " VQ_loss : 122.47545623779297; rec_loss : 8.397117198910564e-06\n",
      " VQ_loss : 125.13617706298828; rec_loss : 1.0252406354993582e-05\n",
      " VQ_loss : 132.59405517578125; rec_loss : 8.628268915344961e-06\n",
      " VQ_loss : 128.18478393554688; rec_loss : 7.969714715727605e-06\n",
      " VQ_loss : 130.60305786132812; rec_loss : 8.74671786732506e-06\n",
      " VQ_loss : 141.88931274414062; rec_loss : 8.277099368569907e-06\n",
      " VQ_loss : 128.91506958007812; rec_loss : 8.116058779705781e-06\n",
      " VQ_loss : 130.2021484375; rec_loss : 7.112932507880032e-06\n",
      " VQ_loss : 129.97474670410156; rec_loss : 6.83470625517657e-06\n",
      " VQ_loss : 130.6624298095703; rec_loss : 7.397746230708435e-06\n",
      " VQ_loss : 133.9423370361328; rec_loss : 6.934312295925338e-06\n",
      " VQ_loss : 131.0821533203125; rec_loss : 8.183953468687832e-06\n",
      " VQ_loss : 131.31008911132812; rec_loss : 8.41243308968842e-06\n",
      " VQ_loss : 134.55303955078125; rec_loss : 6.852978003735188e-06\n",
      " VQ_loss : 132.78457641601562; rec_loss : 6.282916274358286e-06\n",
      " VQ_loss : 126.03269958496094; rec_loss : 7.214151537482394e-06\n",
      " VQ_loss : 134.79098510742188; rec_loss : 6.503632903331891e-06\n",
      " VQ_loss : 133.46493530273438; rec_loss : 5.935130502621178e-06\n",
      " VQ_loss : 135.71133422851562; rec_loss : 6.194429261086043e-06\n",
      " VQ_loss : 144.33953857421875; rec_loss : 6.008535820001271e-06\n",
      " VQ_loss : 139.86862182617188; rec_loss : 8.025230272323824e-06\n",
      " VQ_loss : 145.54249572753906; rec_loss : 5.839190180267906e-06\n",
      " VQ_loss : 137.2242431640625; rec_loss : 6.78990681990399e-06\n",
      " VQ_loss : 141.8189697265625; rec_loss : 5.8842751968768425e-06\n",
      " VQ_loss : 129.994873046875; rec_loss : 9.202033652400132e-06\n",
      " VQ_loss : 134.35858154296875; rec_loss : 6.6423976932128426e-06\n",
      " VQ_loss : 138.92730712890625; rec_loss : 5.604100351774832e-06\n",
      " VQ_loss : 136.0758056640625; rec_loss : 5.119761226524133e-06\n",
      " VQ_loss : 134.1802978515625; rec_loss : 5.030885404266883e-06\n",
      " VQ_loss : 135.81689453125; rec_loss : 6.6824713940150104e-06\n",
      " VQ_loss : 132.5704345703125; rec_loss : 5.01813110531657e-06\n",
      " VQ_loss : 134.76150512695312; rec_loss : 5.019037871534238e-06\n",
      " VQ_loss : 132.55874633789062; rec_loss : 7.401198672596365e-06\n",
      " VQ_loss : 137.85391235351562; rec_loss : 5.1779734349111095e-06\n",
      " VQ_loss : 147.37669372558594; rec_loss : 7.3265305218228605e-06\n",
      " VQ_loss : 138.35562133789062; rec_loss : 6.797783498768695e-06\n",
      " VQ_loss : 128.31788635253906; rec_loss : 5.837430762767326e-06\n",
      " VQ_loss : 134.70411682128906; rec_loss : 5.879079253645614e-06\n",
      " VQ_loss : 125.55406188964844; rec_loss : 6.194668003445258e-06\n",
      " VQ_loss : 124.5041275024414; rec_loss : 4.930839622829808e-06\n",
      " VQ_loss : 133.83224487304688; rec_loss : 5.188465365790762e-06\n",
      " VQ_loss : 135.08876037597656; rec_loss : 4.560428351396695e-06\n",
      " VQ_loss : 146.1413116455078; rec_loss : 6.587361895071808e-06\n",
      " VQ_loss : 133.55494689941406; rec_loss : 4.287108822609298e-06\n",
      " VQ_loss : 130.32603454589844; rec_loss : 4.416908268467523e-06\n",
      " VQ_loss : 146.523681640625; rec_loss : 6.667649358860217e-06\n",
      " VQ_loss : 126.05846405029297; rec_loss : 4.210430688544875e-06\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 在训练循环中使用数据加载器\n",
    "for i in range(50000):\n",
    "    data = next(iter(dataloader)).cuda()\n",
    "    # 在这里进行你的训练...\n",
    "    # print(data.shape)\n",
    "    output_image,loss_vq = model(data)\n",
    "\n",
    "    if loss_vq.dim() > 0:  # 检查loss_vq_G是否为标量\n",
    "            loss_vq = loss_vq.mean()\n",
    "    optimizer_vq.zero_grad()\n",
    "    loss_vq.backward(retain_graph=True)\n",
    "    optimizer_vq.step()\n",
    "\n",
    "    loss_rec = F.mse_loss(data,output_image)\n",
    "    if loss_rec.dim() > 0:  # 检查loss_rec_G是否为标量\n",
    "            loss_rec = loss_rec.mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss_rec.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if(i%100==0):\n",
    "        print(f\" VQ_loss : {loss_vq}; rec_loss : {loss_rec}\")\n",
    "    if(i%1000==0):\n",
    "        # 假设 'model' 是你的神经网络模型实例，'i' 是当前的训练轮数\n",
    "        torch.save(model.state_dict(), f'./model_weight/2/model_weights_epoch_{i}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = {}  # 用于存储每层的输出\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    layer_name = str(module)\n",
    "    layer_outputs[layer_name] = output\n",
    "\n",
    "for name, layer in model.named_children():\n",
    "    layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# 假设 input_image 是您的输入图像\n",
    "model(input_image)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def save_layer_output(output, layer_name, save_dir):\n",
    "    # 将输出转换为可视化的形式\n",
    "    # 注意：这里的转换方式可能需要根据您的具体情况调整\n",
    "    output = output.detach().cpu().numpy()\n",
    "    if output.ndim == 4:  # 对于卷积层的输出\n",
    "        # 取第一个样本的第一个特征映射\n",
    "        output_img = output[0, 0]\n",
    "    else:  # 对于全连接层等的输出\n",
    "        # 将输出转换为一个方形图像\n",
    "        side_length = int(np.ceil(np.sqrt(output.size)))\n",
    "        output_img = np.reshape(output, (side_length, side_length))\n",
    "\n",
    "    # 保存输出图像\n",
    "    plt.imsave(f'{save_dir}/{layer_name}.png', output_img, cmap='gray')\n",
    "\n",
    "# 遍历并保存每层的输出\n",
    "for layer_name, output in layer_outputs.items():\n",
    "    save_layer_output(output, layer_name, 'output_images')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
