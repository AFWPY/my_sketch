{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import functools\n",
    "import copy\n",
    "from model.Component_Attention_Module import ComponentAttentionModule\n",
    "\n",
    "from model.VectorQuantizer import  VectorQuantizer\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(Generator,self).__init__()\n",
    "\n",
    "        # codebook使用VQ-VAE进行编码\n",
    "        num_embeddings = 8192 # 嵌入向量数量，过多容易过拟合，过少容易欠拟合\n",
    "        embedding_dim = 512*8*8 # 512*1*1\n",
    "        commitment_cost = 0.25\n",
    "        self.vq = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
    "\n",
    "        # MHA多头注意力机制，输入input和vq生成的Query\n",
    "        self.MHA = ComponentAttentionModule()\n",
    "\n",
    "        # construct unet structure\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True,vq=self.vq,HMA=self.MHA)  # add the innermost layer\n",
    "        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n",
    "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout,nodown=True)\n",
    "        # gradually reduce the number of filters from ngf * 8 to ngf\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n",
    "            unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout,nodown=True)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n",
    "            unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout,nodown=True)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n",
    "            unet_block = UnetSkipConnectionBlock(ngf, ngf, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout,nodown=True)\n",
    "        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)  # add the outermost layer\n",
    "\n",
    "    def forward(self, input_s):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "        return self.model(input_s)\n",
    "\n",
    "class UnetSkipConnectionBlock(nn.Module):\n",
    "    \"\"\"Defines the Unet submodule with skip connection.\n",
    "        X -------------------identity----------------------\n",
    "        |-- downsampling -- |submodule| -- upsampling --|\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False,HMA = None,vq = None,nodown = None):\n",
    "        \"\"\"Construct a Unet submodule with skip connections.\n",
    "\n",
    "        Parameters:\n",
    "            outer_nc (int) -- the number of filters in the outer conv layer\n",
    "            inner_nc (int) -- the number of filters in the inner conv layer\n",
    "            input_nc (int) -- the number of channels in input images/features\n",
    "            submodule (UnetSkipConnectionBlock) -- previously defined submodules\n",
    "            outermost (bool)    -- if this module is the outermost module\n",
    "            innermost (bool)    -- if this module is the innermost module\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers.\n",
    "        \"\"\"\n",
    "        super(UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        self.innermost = innermost\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        if input_nc is None:\n",
    "            input_nc = outer_nc\n",
    "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=use_bias)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = norm_layer(inner_nc)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = norm_layer(outer_nc)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "\n",
    "            self.down_s = nn.Sequential(*[copy.deepcopy(layer) for layer in down])\n",
    "            self.up = nn.Sequential(*up)\n",
    "            self.submodule  = submodule\n",
    "        elif innermost:\n",
    "             # 原有的下采样层\n",
    "            down = [downrelu, downconv]\n",
    "            self.down_s = nn.Sequential(*[copy.deepcopy(layer) for layer in down])       \n",
    "            # 定义上采样层\n",
    "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            self.up = nn.Sequential(*up)\n",
    "            #生成codebook中最相近的vq\n",
    "            self.vq = vq\n",
    "            # MHA多头注意力机制，输入input和vq生成的Query\n",
    "            self.MHA = HMA\n",
    "\n",
    "        elif nodown:\n",
    "            nodownconv = nn.Conv2d(input_nc, inner_nc, kernel_size=3,\n",
    "                     stride=1, padding=1, bias=use_bias)\n",
    "            noupconv = nn.Conv2d(inner_nc * 2, outer_nc, kernel_size=3,\n",
    "                     stride=1, padding=1, bias=use_bias)\n",
    "            \n",
    "            down = [downrelu, nodownconv, downnorm]\n",
    "            up = [uprelu, noupconv, upnorm]\n",
    "            self.down_s = nn.Sequential(*[copy.deepcopy(layer) for layer in down])\n",
    "            self.up = nn.Sequential(*up)\n",
    "            self.submodule  = submodule\n",
    "\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            self.down_s = nn.Sequential(*[copy.deepcopy(layer) for layer in down])\n",
    "            self.up = nn.Sequential(*up)\n",
    "            self.submodule  = submodule\n",
    "\n",
    "\n",
    "    def forward(self, style):\n",
    "        if self.outermost:\n",
    "            down_s = self.down_s(style)\n",
    "            submoduled_s,vq_loss_s= self.submodule(down_s)\n",
    "            up_s = self.up(submoduled_s)\n",
    "            return up_s,vq_loss_s\n",
    "        else:   # add skip connections\n",
    "            if self.innermost:\n",
    "                # 在最内层先进行下采样\n",
    "                down_s = self.down_s(style)            \n",
    "                # 然后并行地执行MHA和VQ操作               \n",
    "                query_s ,vq_loss_s  = self.vq(down_s)\n",
    "                MHA_s = self.MHA(down_s,query_s)\n",
    "                # 执行上采样\n",
    "                up_s = self.up(MHA_s)\n",
    "                return torch.cat([style, up_s], 1),vq_loss_s\n",
    "            else:\n",
    "                down_s = self.down_s(style)\n",
    "                submoduled_s,vq_loss_s = self.submodule(down_s)\n",
    "                up_s = self.up(submoduled_s)\n",
    "                # 对于非最内层，添加skip连接和子模块            \n",
    "                return torch.cat([style, up_s], 1) ,vq_loss_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.image_paths = root\n",
    "        self.imgs = self.read_file(self.image_paths)\n",
    "        self.transform = transform\n",
    "\n",
    "    def read_file(self, path):\n",
    "        \"\"\"从文件夹中读取数据\"\"\"\n",
    "        files_list = os.listdir(path)\n",
    "        file_path_list = [os.path.join(path, img) for img in files_list]\n",
    "        file_path_list.sort()  # 如果你需要特定的顺序则保留这一行\n",
    "        return file_path_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)  # 返回图片列表的长度\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.imgs[index]  # 使用图片列表中的路径\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个转换操作，比如转换成张量并且归一化\n",
    "transform = transforms.Compose([transforms.Resize((256, 256)),\n",
    "                                          transforms.ToTensor()])\n",
    "\n",
    "train_data_root = \"./data/CUHK/trainB\"\n",
    "\n",
    "# 创建数据集\n",
    "dataset = CustomDataset(root=train_data_root, transform=transform)\n",
    "\n",
    "# 创建数据加载器\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "dataiter = iter(dataloader)\n",
    "images = next(dataiter)\n",
    "\n",
    "# 打印图像和标签的形状\n",
    "print('Images shape:', images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "model = Generator(input_nc=3, output_nc=3,num_downs=8)\n",
    "\n",
    "\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# 提取 model.vq 的参数\n",
    "vq_params = set(model.vq.parameters())\n",
    "# 提取除 model.vq 之外的所有参数\n",
    "other_params = filter(lambda p: p not in vq_params, model.parameters())\n",
    "\n",
    "optimizer_vq = optim.Adam(vq_params, lr=learning_rate, amsgrad=False)\n",
    "optimizer = optim.Adam(other_params, lr=learning_rate, amsgrad=False)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model,device_ids=[0, 1])\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vq_gan import ResnetGenerator\n",
    "model = ResnetGenerator(3,3,n_blocks=6)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在训练循环中使用数据加载器\n",
    "for i in range(50000):\n",
    "    data = next(iter(dataloader)).cuda()\n",
    "    # 在这里进行你的训练...\n",
    "    # print(data.shape)\n",
    "    output_image,loss_vq = model(data)\n",
    "\n",
    "    if loss_vq.dim() > 0:  # 检查loss_vq_G是否为标量\n",
    "            loss_vq = loss_vq.mean()\n",
    "    optimizer_vq.zero_grad()\n",
    "    loss_vq.backward(retain_graph=True)\n",
    "    optimizer_vq.step()\n",
    "\n",
    "    loss_rec = F.mse_loss(data,output_image)\n",
    "    if loss_rec.dim() > 0:  # 检查loss_rec_G是否为标量\n",
    "            loss_rec = loss_rec.mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss_rec.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if(i%100==0):\n",
    "        print(f\" VQ_loss : {loss_vq}; rec_loss : {loss_rec}\")\n",
    "    if(i%1000==0):\n",
    "        # 假设 'model' 是你的神经网络模型实例，'i' 是当前的训练轮数\n",
    "        torch.save(model.state_dict(), f'./model_weight/2/model_weights_epoch_{i}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = {}  # 用于存储每层的输出\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    layer_name = str(module)\n",
    "    layer_outputs[layer_name] = output\n",
    "\n",
    "for name, layer in model.named_children():\n",
    "    layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# 假设 input_image 是您的输入图像\n",
    "model(input_image)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def save_layer_output(output, layer_name, save_dir):\n",
    "    # 将输出转换为可视化的形式\n",
    "    # 注意：这里的转换方式可能需要根据您的具体情况调整\n",
    "    output = output.detach().cpu().numpy()\n",
    "    if output.ndim == 4:  # 对于卷积层的输出\n",
    "        # 取第一个样本的第一个特征映射\n",
    "        output_img = output[0, 0]\n",
    "    else:  # 对于全连接层等的输出\n",
    "        # 将输出转换为一个方形图像\n",
    "        side_length = int(np.ceil(np.sqrt(output.size)))\n",
    "        output_img = np.reshape(output, (side_length, side_length))\n",
    "\n",
    "    # 保存输出图像\n",
    "    plt.imsave(f'{save_dir}/{layer_name}.png', output_img, cmap='gray')\n",
    "\n",
    "# 遍历并保存每层的输出\n",
    "for layer_name, output in layer_outputs.items():\n",
    "    save_layer_output(output, layer_name, 'output_images')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
